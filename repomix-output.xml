This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where content has been compressed (code blocks are separated by â‹®---- delimiter).

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: **/*.min.js, **/*.min.css, **/*.svg, src/scripts/libs/**, vendor/**, **/*.lock, **/__pycache__/**
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Content has been compressed - code blocks are separated by â‹®---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
database/
  __init__.py
  connections.py
  models.py
  schema_analyzer.py
query_engine/
  __init__.py
  nl_processor.py
  query_validator.py
ui/
  __init__.py
  components.py
utils/
  __init__.py
  helpers.py
.gitignore
app.py
config.py
generate_key.py
main.py
requirements.txt
test_imports.py
test_nl_processor.py
test_phase_2_1.py
test_phase_2_2.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="database/__init__.py">
__all__ = [
</file>

<file path="database/connections.py">
class DatabaseManager
â‹®----
def __init__(self)
â‹®----
def _get_cipher(self) -> Optional[Fernet]
â‹®----
"""Initialize encryption for connection strings"""
â‹®----
# The key from .env is already properly formatted for Fernet
# Don't double-encode it
â‹®----
def add_connection(self, config: ConnectionConfig) -> bool
â‹®----
"""Add a new database connection"""
â‹®----
# Encrypt connection string if cipher is available
connection_string = config.connection_string
â‹®----
connection_string = self.cipher.encrypt(connection_string.encode()).decode()
â‹®----
engine = create_engine(
â‹®----
# Test connection
â‹®----
def remove_connection(self, name: str) -> bool
â‹®----
"""Remove a database connection"""
â‹®----
def set_active_connection(self, name: str) -> bool
â‹®----
"""Set the active database connection"""
â‹®----
def get_active_engine(self) -> Optional[sa.Engine]
â‹®----
"""Get the currently active database engine"""
â‹®----
def list_connections(self) -> List[str]
â‹®----
"""List all available connection names"""
â‹®----
def test_connection(self, connection_string: str) -> tuple[bool, str]
â‹®----
"""Test a database connection string"""
â‹®----
engine = create_engine(connection_string)
â‹®----
result = conn.execute(text("SELECT 1"))
â‹®----
def execute_query(self, query: str, params: Optional[Dict] = None) -> pd.DataFrame
â‹®----
"""Execute a query and return results as DataFrame"""
engine = self.get_active_engine()
â‹®----
# Set query timeout
â‹®----
result = pd.read_sql(
â‹®----
# Limit result size
â‹®----
result = result.head(MAX_RESULT_ROWS)
â‹®----
def get_database_type(self) -> Optional[DatabaseType]
â‹®----
"""Get the type of the active database"""
â‹®----
dialect_name = engine.dialect.name.lower()
â‹®----
def create_sample_sqlite_db(self, file_path: str = "sample_data/sample.db") -> bool
â‹®----
"""Create a sample SQLite database for testing"""
â‹®----
conn = sqlite3.connect(file_path)
cursor = conn.cursor()
â‹®----
# Create sample tables
â‹®----
# Insert sample data
customers_data = [
â‹®----
orders_data = [
â‹®----
products_data = [
</file>

<file path="database/models.py">
class DatabaseType(Enum)
â‹®----
SQLITE = "sqlite"
MYSQL = "mysql"
POSTGRESQL = "postgresql"
â‹®----
@dataclass
class ColumnInfo
â‹®----
name: str
data_type: str
is_nullable: bool
is_primary_key: bool
is_foreign_key: bool
foreign_key_table: Optional[str] = None
foreign_key_column: Optional[str] = None
default_value: Optional[Any] = None
max_length: Optional[int] = None
description: Optional[str] = None
â‹®----
@dataclass
class TableInfo
â‹®----
columns: List[ColumnInfo]
row_count: Optional[int] = None
â‹®----
def get_column(self, name: str) -> Optional[ColumnInfo]
â‹®----
def get_primary_keys(self) -> List[ColumnInfo]
â‹®----
def get_foreign_keys(self) -> List[ColumnInfo]
â‹®----
@dataclass
class DatabaseSchema
â‹®----
database_name: str
database_type: DatabaseType
tables: List[TableInfo]
connection_string: str
â‹®----
def get_table(self, name: str) -> Optional[TableInfo]
â‹®----
def get_table_names(self) -> List[str]
â‹®----
def to_context_string(self, max_tables: int = 20) -> str
â‹®----
"""Convert schema to string format for LLM context"""
context_parts = [f"Database: {self.database_name} ({self.database_type.value})"]
â‹®----
tables_to_include = self.tables[:max_tables]
â‹®----
col_info = f"  - {col.name}: {col.data_type}"
â‹®----
@dataclass
class ConnectionConfig
â‹®----
db_type: str
â‹®----
is_active: bool = True
</file>

<file path="database/schema_analyzer.py">
class SchemaAnalyzer
â‹®----
def __init__(self, engine)
â‹®----
def analyze_database(self, database_name: str = None) -> DatabaseSchema
â‹®----
"""Enhanced database schema analysis with relationships and statistics"""
â‹®----
# Get database type
db_type = self._get_database_type()
â‹®----
# Get database name
â‹®----
database_name = self._get_database_name()
â‹®----
# Get all tables with enhanced analysis
table_names = self.inspector.get_table_names()
tables = []
â‹®----
# Build relationship map first
â‹®----
table_info = self._analyze_table_enhanced(table_name)
â‹®----
# Sort tables by importance (tables with more relationships first)
â‹®----
def _get_database_type(self) -> DatabaseType
â‹®----
"""Determine database type from engine"""
dialect_name = self.engine.dialect.name.lower()
â‹®----
# Default to SQLite for unknown types
â‹®----
def _get_database_name(self) -> str
â‹®----
"""Extract database name from connection"""
â‹®----
# For SQLite, use the filename
â‹®----
db_path = str(self.engine.url.database)
â‹®----
# For other databases, use the database name from URL
â‹®----
def _build_relationship_map(self, table_names: List[str])
â‹®----
"""Build a comprehensive map of table relationships"""
relationships = defaultdict(list)
â‹®----
# Get foreign keys
fk_constraints = self.inspector.get_foreign_keys(table_name)
â‹®----
source_table = table_name
target_table = fk['referred_table']
â‹®----
# Store bidirectional relationships
â‹®----
def _analyze_table_enhanced(self, table_name: str) -> TableInfo
â‹®----
"""Enhanced table analysis with statistics and relationships"""
â‹®----
# Get basic column information
columns_info = self.inspector.get_columns(table_name)
â‹®----
# Get constraints
pk_constraint = self.inspector.get_pk_constraint(table_name)
primary_keys = pk_constraint.get('constrained_columns', [])
â‹®----
foreign_keys = {}
â‹®----
# Get unique constraints
â‹®----
unique_constraints = self.inspector.get_unique_constraints(table_name)
unique_columns = set()
â‹®----
# Build enhanced column information
columns = []
â‹®----
col_name = col_info['name']
â‹®----
# Get column statistics
col_stats = self._get_column_statistics(table_name, col_name)
â‹®----
column = ColumnInfo(
â‹®----
# Add unique constraint info
â‹®----
# Get enhanced table statistics
row_count = self._get_table_row_count(table_name)
â‹®----
# Generate table description
table_description = self._generate_table_description(table_name, columns, row_count)
â‹®----
def _normalize_data_type(self, data_type: str) -> str
â‹®----
"""Normalize database-specific data types to common types"""
data_type = data_type.upper()
â‹®----
# Common mappings
type_mappings = {
â‹®----
# Extract base type (remove size specifications)
base_type = re.sub(r'\([^)]*\)', '', data_type).strip()
â‹®----
def _extract_max_length(self, data_type: str) -> Optional[int]
â‹®----
"""Extract maximum length from data type specification"""
match = re.search(r'\((\d+)\)', data_type)
â‹®----
def _get_column_statistics(self, table_name: str, column_name: str) -> Dict[str, Any]
â‹®----
"""Get basic statistics for a column"""
â‹®----
# Get distinct count and null count
query = f"""
result = conn.execute(text(query)).fetchone()
â‹®----
def _generate_column_description(self, column_name: str, stats: Dict[str, Any]) -> str
â‹®----
"""Generate intelligent description for a column"""
descriptions = []
â‹®----
# Infer purpose from column name
name_lower = column_name.lower()
â‹®----
# Add statistics info
â‹®----
total = stats.get('total_count', 0)
distinct = stats.get('distinct_count', 0)
â‹®----
uniqueness = distinct / total
â‹®----
def _generate_table_description(self, table_name: str, columns: List[ColumnInfo], row_count: int) -> str
â‹®----
"""Generate intelligent description for a table"""
â‹®----
# Infer purpose from table name
name_lower = table_name.lower()
â‹®----
# Add structural info
fk_count = len([c for c in columns if c.is_foreign_key])
â‹®----
def _get_table_row_count(self, table_name: str) -> Optional[int]
â‹®----
"""Get approximate row count for a table"""
â‹®----
result = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
â‹®----
def get_table_relationships(self, table_name: str) -> List[Dict]
â‹®----
"""Get all relationships for a specific table"""
â‹®----
def get_related_tables(self, table_name: str) -> List[str]
â‹®----
"""Get list of tables related to the given table"""
relationships = self._table_relationships.get(table_name, [])
related_tables = set()
â‹®----
def get_sample_data(self, table_name: str, limit: int = 5) -> pd.DataFrame
â‹®----
"""Get sample data from a table"""
â‹®----
query = f"SELECT * FROM {table_name} LIMIT {limit}"
â‹®----
def search_tables(self, search_term: str) -> List[str]
â‹®----
"""Search for tables by name"""
â‹®----
search_term = search_term.lower()
â‹®----
def search_columns(self, search_term: str) -> Dict[str, List[str]]
â‹®----
"""Search for columns across all tables"""
results = {}
â‹®----
columns = self.inspector.get_columns(table_name)
matching_columns = [
â‹®----
def suggest_joins(self, tables: List[str]) -> List[Dict[str, str]]
â‹®----
"""Suggest possible JOIN operations between tables"""
joins = []
â‹®----
# Check if table1 has FK to table2
relationships = self._table_relationships.get(table1, [])
â‹®----
# Check if table2 has FK to table1
relationships = self._table_relationships.get(table2, [])
â‹®----
def get_schema_context_optimized(self, max_tokens: int = 3000) -> str
â‹®----
"""Get optimized schema context for LLM with token limit consideration"""
â‹®----
schema = self.analyze_database()
â‹®----
# Start with basic info
context_parts = [
â‹®----
# Estimate tokens (rough: 1 token â‰ˆ 4 characters)
current_length = len("\n".join(context_parts))
target_length = max_tokens * 4  # Convert tokens to characters
â‹®----
# Sort tables by importance
important_tables = sorted(schema.tables,
â‹®----
(t.row_count or 0) / 1000  # Favor tables with more data
â‹®----
# Add tables until we hit token limit
â‹®----
table_context = f"\n\nTable: {table.name}"
â‹®----
# Add essential columns first (PK, FK, then others)
essential_columns = [c for c in table.columns if c.is_primary_key or c.is_foreign_key]
other_columns = [c for c in table.columns if not c.is_primary_key and not c.is_foreign_key]
â‹®----
col_info = f"\n  - {col.name}: {col.data_type}"
â‹®----
# Check if adding this would exceed limit
â‹®----
# Add other columns if space permits
for col in other_columns[:5]:  # Limit to 5 additional columns per table
â‹®----
# Add row count if available
â‹®----
# Add relationship information if space permits
if current_length < target_length * 0.8:  # Use 80% of available space
relationships = []
for table in important_tables[:5]:  # Top 5 tables only
related = self.get_related_tables(table.name)
â‹®----
rel_context = f"\n\nKey Relationships:\n" + "\n".join(relationships)
â‹®----
def search_schema(self, search_term: str) -> Dict[str, Any]
â‹®----
"""Advanced schema search functionality"""
â‹®----
results = {
â‹®----
# Search tables
â‹®----
# Search columns
â‹®----
# Search relationships
â‹®----
# Generate suggestions
â‹®----
# Fuzzy matching suggestions
all_terms = []
â‹®----
# Simple fuzzy matching
suggestions = [term for term in all_terms if search_term in term.lower()]
â‹®----
def get_join_suggestions_for_query(self, mentioned_tables: List[str]) -> List[str]
â‹®----
"""Get JOIN suggestions for a natural language query"""
â‹®----
join_suggestions = []
joins = self.suggest_joins(mentioned_tables)
â‹®----
suggestion = f"{join['type']} {join['table2']} ON {join['condition']}"
â‹®----
# Start with basic info
â‹®----
# Estimate tokens (rough: 1 token â‰ˆ 4 characters)
â‹®----
target_length = max_tokens * 4  # Convert tokens to characters
â‹®----
# Sort tables by importance
â‹®----
len(t.get_foreign_keys()) * 3 +  # Tables with FKs are more important
len(t.get_primary_keys()) * 2 +   # Tables with PKs are important
(t.row_count or 0) / 1000         # Favor tables with more data
â‹®----
# Add tables until we hit token limit
â‹®----
# Add essential columns first (PK, FK, then others)
â‹®----
flags = []
â‹®----
# Check if adding this would exceed limit
â‹®----
# Add other columns if space permits
for col in other_columns[:5]:  # Limit to 5 additional columns per table
â‹®----
# Add row count if available
â‹®----
# Check if we can fit this table
â‹®----
# Add relationship information if space permits
if current_length < target_length * 0.8:  # Use 80% of available space
â‹®----
for table in important_tables[:5]:  # Top 5 tables only
â‹®----
# Search tables
â‹®----
# Search columns
â‹®----
# Search relationships
â‹®----
related_tables = self.get_related_tables(table_name)
â‹®----
# Generate suggestions
â‹®----
# Fuzzy matching suggestions
â‹®----
# Simple fuzzy matching
</file>

<file path="query_engine/__init__.py">
__all__ = ['NLProcessor', 'QueryValidator', 'QueryType', 'QueryIntent', 'QueryResult']
</file>

<file path="query_engine/nl_processor.py">
# query_engine/nl_processor.py
â‹®----
class QueryType(Enum)
â‹®----
SELECT = "select"
INSERT = "insert"
UPDATE = "update"
DELETE = "delete"
CREATE = "create"
DROP = "drop"
UNKNOWN = "unknown"
â‹®----
@dataclass
class QueryIntent
â‹®----
query_type: QueryType
confidence: float
tables_mentioned: List[str]
columns_mentioned: List[str]
requires_joins: bool
complexity_score: int
â‹®----
@dataclass
class QueryResult
â‹®----
sql_query: str
intent: QueryIntent
explanation: str
warnings: List[str]
is_safe: bool
â‹®----
class NLProcessor
â‹®----
def __init__(self, schema: DatabaseSchema, schema_analyzer=None)
â‹®----
self.schema_analyzer = schema_analyzer  # Optional for advanced features
â‹®----
# Configure Gemini
â‹®----
def process_query(self, natural_query: str) -> QueryResult
â‹®----
"""Main entry point - like a smart WordPress AJAX handler"""
â‹®----
# Step 1: Analyze intent (like parsing $_POST data)
intent = self._analyze_intent(natural_query)
â‹®----
# Step 2: Generate SQL (like building database query)
sql_query = self._generate_sql(natural_query, intent)
â‹®----
# Step 3: Validate and add safety checks (like wp_verify_nonce)
is_safe = self._validate_query_safety(sql_query)
warnings = self._generate_warnings(sql_query, intent)
â‹®----
# Step 4: Generate explanation (like admin notices)
explanation = self._explain_query(sql_query, natural_query)
â‹®----
def _analyze_intent(self, query: str) -> QueryIntent
â‹®----
"""Analyze what the user wants to do - like parsing form data"""
â‹®----
# Simple keyword-based classification first
query_lower = query.lower()
â‹®----
# Detect query type (like checking $_POST['action'])
â‹®----
query_type = QueryType.SELECT
â‹®----
query_type = QueryType.INSERT
â‹®----
query_type = QueryType.UPDATE
â‹®----
query_type = QueryType.DELETE
â‹®----
query_type = QueryType.UNKNOWN
â‹®----
# Find mentioned tables (like finding post types in query)
tables_mentioned = []
â‹®----
# Detect if joins are needed (like checking for related post data)
requires_joins = len(tables_mentioned) > 1 or any(word in query_lower for word in ['join', 'with', 'and', 'related'])
â‹®----
# Calculate complexity (like checking query performance)
complexity_score = self._calculate_complexity(query, tables_mentioned, requires_joins)
â‹®----
confidence=0.8,  # We'll make this smarter later
â‹®----
columns_mentioned=[],  # TODO: Implement column detection
â‹®----
def _generate_sql(self, natural_query: str, intent: QueryIntent) -> str
â‹®----
"""Generate SQL using Gemini - like building WP_Query"""
â‹®----
# Build context-aware prompt
prompt = self._build_generation_prompt(natural_query, intent)
â‹®----
# Generate with Gemini (like calling wp_remote_get)
response = self.model.generate_content(prompt)
â‹®----
sql_query = response.text.strip()
â‹®----
# Clean up the response (like sanitizing form input)
sql_query = re.sub(r'```sql\n?', '', sql_query)
sql_query = re.sub(r'```\n?', '', sql_query)
sql_query = sql_query.strip()
â‹®----
def _build_generation_prompt(self, natural_query: str, intent: QueryIntent) -> str
â‹®----
"""Build context-rich prompt - like preparing template data"""
â‹®----
# Get relevant schema context
schema_context = self._get_relevant_schema_context(intent.tables_mentioned)
â‹®----
# Build the prompt (like building template variables)
prompt = f"""You are an expert SQL query generator.
â‹®----
def _get_relevant_schema_context(self, mentioned_tables: List[str]) -> str
â‹®----
"""Get schema context for mentioned tables - like getting post meta"""
â‹®----
# Return a summary of all tables (like get_post_types())
â‹®----
# Build context for specific tables (like get_post_meta for specific posts)
context_parts = []
â‹®----
table = self.schema.get_table(table_name)
â‹®----
flags = []
â‹®----
flag_str = f" ({', '.join(flags)})" if flags else ""
â‹®----
def _validate_query_safety(self, sql_query: str) -> bool
â‹®----
"""Validate query safety - like current_user_can() checks"""
â‹®----
def _generate_warnings(self, sql_query: str, intent: QueryIntent) -> List[str]
â‹®----
"""Generate warnings - like admin_notices"""
warnings = []
â‹®----
def _explain_query(self, sql_query: str, natural_query: str) -> str
â‹®----
"""Generate query explanation - like contextual help text"""
â‹®----
explanation_prompt = f"""Explain this SQL query in simple terms:
â‹®----
response = self.model.generate_content(explanation_prompt)
â‹®----
def _calculate_complexity(self, query: str, tables: List[str], requires_joins: bool) -> int
â‹®----
"""Calculate query complexity score (1-10) - like post query performance"""
score = 1
â‹®----
# Base complexity
â‹®----
# Join complexity
â‹®----
# Aggregation complexity
â‹®----
# Subquery complexity
â‹®----
"""Enhanced intent analysis with smarter table and column detection"""
â‹®----
# Detect query type (existing logic)
â‹®----
# Enhanced table detection with schema search
â‹®----
# Direct table name matching
â‹®----
# Fuzzy matching for partial names
â‹®----
search_results = self._search_schema_for_query(query_lower)
â‹®----
# Enhanced column detection
columns_mentioned = self._detect_mentioned_columns(query_lower, tables_mentioned)
â‹®----
# Smart JOIN detection
requires_joins = self._analyze_join_requirements(query_lower, tables_mentioned)
â‹®----
# Enhanced complexity calculation
complexity_score = self._calculate_enhanced_complexity(query, tables_mentioned, columns_mentioned, requires_joins)
â‹®----
def _search_schema_for_query(self, query_lower: str) -> Dict[str, List[str]]
â‹®----
"""Search schema for relevant tables based on query content"""
â‹®----
results = {
â‹®----
# Business logic keywords to table mapping
business_keywords = {
â‹®----
# Check for business context
â‹®----
# Remove duplicates
â‹®----
def _detect_mentioned_columns(self, query_lower: str, tables_mentioned: List[str]) -> List[str]
â‹®----
"""Detect column names mentioned in natural language query"""
â‹®----
columns_mentioned = []
â‹®----
# Common column name patterns
column_keywords = {
â‹®----
# Search for column keywords in query
â‹®----
# Check if these columns exist in mentioned tables
â‹®----
def _analyze_join_requirements(self, query_lower: str, tables_mentioned: List[str]) -> bool
â‹®----
"""Analyze if the query requires JOINs"""
â‹®----
# Explicit JOIN keywords
â‹®----
# Multiple tables mentioned
â‹®----
# Relationship keywords
relationship_words = ['customer orders', 'order items', 'product sales', 'user purchases']
â‹®----
def _calculate_enhanced_complexity(self, query: str, tables: List[str], columns: List[str], requires_joins: bool) -> int
â‹®----
"""Enhanced complexity calculation"""
â‹®----
# JOIN complexity
â‹®----
agg_words = ['sum', 'count', 'avg', 'max', 'min', 'total', 'average', 'top', 'bottom']
agg_count = sum(1 for word in agg_words if word in query_lower)
â‹®----
# Grouping complexity
â‹®----
# Sorting complexity
â‹®----
# Date/time complexity
â‹®----
def _calculate_confidence(self, query_lower: str, tables: List[str], columns: List[str]) -> float
â‹®----
"""Calculate confidence score for intent analysis"""
â‹®----
confidence = 0.5  # Base confidence
â‹®----
# Boost confidence for clear table matches
â‹®----
# Boost confidence for column matches
â‹®----
# Boost confidence for clear SQL keywords
sql_keywords = ['select', 'show', 'get', 'find', 'list']
â‹®----
"""Enhanced prompt building with smart context"""
â‹®----
# Get optimized schema context based on mentioned tables
â‹®----
schema_context = self._get_focused_schema_context(intent.tables_mentioned)
â‹®----
schema_context = self._get_general_schema_context()
â‹®----
# Get JOIN suggestions if needed
join_suggestions = []
â‹®----
# This would use the schema analyzer's suggest_joins method
joins = self.schema_analyzer.suggest_joins(intent.tables_mentioned) if hasattr(self, 'schema_analyzer') else []
join_suggestions = [f"Consider: {j['type']} {j['table2']} ON {j['condition']}" for j in joins[:2]]
â‹®----
# Build enhanced prompt
prompt = f"""You are an expert SQL query generator with deep database knowledge.
â‹®----
def _get_focused_schema_context(self, mentioned_tables: List[str]) -> str
â‹®----
"""Get detailed context for specific tables mentioned in query"""
â‹®----
# Add table description if available
â‹®----
# Add all columns with detailed info
â‹®----
# Add row count if available
â‹®----
# Add related tables that might be useful
related_tables = []
â‹®----
# Get tables related to this one
â‹®----
related = self.schema_analyzer.get_related_tables(table_name)
â‹®----
# Include context for related tables (abbreviated)
â‹®----
for table_name in related_tables[:3]:  # Limit to 3 related tables
â‹®----
key_columns = [c for c in table.columns if c.is_primary_key or c.is_foreign_key]
col_summary = ", ".join([f"{c.name}({c.data_type})" for c in key_columns[:3]])
â‹®----
def _get_general_schema_context(self) -> str
â‹®----
"""Get general schema context when no specific tables are mentioned"""
â‹®----
# Use the schema's built-in context method with optimization
â‹®----
"""Enhanced warning generation with performance and optimization hints"""
â‹®----
# Safety warnings
â‹®----
# Complexity warnings
â‹®----
# JOIN warnings
â‹®----
join_count = len(intent.tables_mentioned)
â‹®----
# Query pattern warnings
sql_upper = sql_query.upper()
â‹®----
if sql_upper.count('SELECT') > 1:  # Subqueries detected
â‹®----
# Table size warnings (if we have row count data)
large_tables = []
â‹®----
def suggest_query_improvements(self, sql_query: str, intent: QueryIntent) -> List[str]
â‹®----
"""Suggest improvements for the generated query"""
â‹®----
suggestions = []
â‹®----
# Index suggestions
â‹®----
# Query optimization suggestions
â‹®----
# Alternative approaches
â‹®----
def generate_query_alternatives(self, natural_query: str, intent: QueryIntent) -> List[str]
â‹®----
"""Generate alternative ways to write the same query"""
â‹®----
alternatives = []
â‹®----
# If it's a simple SELECT, suggest variations
â‹®----
table_name = intent.tables_mentioned[0]
â‹®----
# Suggest different column selections
â‹®----
# Suggest with conditions
â‹®----
# Suggest with ordering
â‹®----
# If it involves JOINs, suggest different JOIN types
â‹®----
"""Enhanced query explanation with performance insights"""
â‹®----
# Enhanced explanation prompt
explanation_prompt = f"""Explain this SQL query in simple, business-friendly terms:
â‹®----
# Fallback explanation
â‹®----
def _generate_fallback_explanation(self, sql_query: str, natural_query: str) -> str
â‹®----
"""Generate a basic explanation when AI explanation fails"""
â‹®----
explanation_parts = [f"This query responds to: '{natural_query}'"]
â‹®----
# Determine operation type
â‹®----
# Identify tables
tables_in_query = []
â‹®----
# Identify JOINs
â‹®----
# Identify aggregations
</file>

<file path="query_engine/query_validator.py">
class QueryValidator
â‹®----
def __init__(self, schema: DatabaseSchema)
â‹®----
def validate_query(self, sql_query: str) -> Dict[str, Any]
â‹®----
"""Comprehensive query validation"""
â‹®----
validation_result = {
â‹®----
# 1. Syntax validation
syntax_result = self._validate_syntax(sql_query)
â‹®----
# 2. Safety validation
â‹®----
# 3. Table validation
table_validation = validate_table_names(sql_query, self.schema.get_table_names())
â‹®----
# 4. Complexity analysis
complexity_analysis = estimate_query_complexity(sql_query)
â‹®----
# 5. Overall validity
â‹®----
def _validate_syntax(self, sql_query: str) -> Dict[str, Any]
â‹®----
"""Validate SQL syntax"""
â‹®----
parsed = sqlparse.parse(sql_query)
â‹®----
# Check for basic SQL structure
</file>

<file path="ui/__init__.py">
__all__ = ['UIComponents']
</file>

<file path="ui/components.py">
# ui/components.py - Complete rewrite for Phase 3
â‹®----
class UIComponents
â‹®----
@staticmethod
    def render_header()
â‹®----
"""Render the main application header"""
â‹®----
@staticmethod
    def render_sidebar_navigation()
â‹®----
"""Render sidebar navigation menu"""
â‹®----
# Quick stats about current session
â‹®----
query_count = len(st.session_state.query_history)
successful_queries = len([q for q in st.session_state.query_history if q.get('success', False)])
â‹®----
# Quick actions
â‹®----
@staticmethod
    def render_connection_manager()
â‹®----
"""Enhanced database connection manager"""
â‹®----
# Show active connections
connections = st.session_state.db_manager.list_connections()
active_connection = st.session_state.db_manager.active_connection
â‹®----
is_active = conn == active_connection
status = "ğŸŸ¢" if is_active else "âšª"
â‹®----
# Connection form in expander
â‹®----
@staticmethod
    def render_connection_form()
â‹®----
"""Database connection form"""
â‹®----
conn_name = st.text_input("Connection Name", placeholder="My Database")
db_type = st.selectbox("Database Type", ["SQLite", "MySQL", "PostgreSQL"])
â‹®----
db_file = st.text_input("Database File", placeholder="path/to/database.db")
â‹®----
conn_string = f"sqlite:///{db_file}" if db_file else ""
â‹®----
host = st.text_input("Host", value="localhost")
user = st.text_input("Username")
â‹®----
port = st.number_input("Port", value=3306)
password = st.text_input("Password", type="password")
â‹®----
database = st.text_input("Database Name")
conn_string = f"mysql+pymysql://{user}:{password}@{host}:{port}/{database}" if all([user, password, database]) else ""
â‹®----
else:  # PostgreSQL
â‹®----
port = st.number_input("Port", value=5432)
â‹®----
conn_string = f"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}" if all([user, password, database]) else ""
â‹®----
# Test and add connection
â‹®----
config = ConnectionConfig(
â‹®----
success = st.session_state.db_manager.add_connection(config)
â‹®----
@staticmethod
    def render_schema_explorer()
â‹®----
"""Enhanced schema explorer with search and relationships"""
â‹®----
analyzer = st.session_state.schema_analyzer
schema = st.session_state.schema
â‹®----
# Search functionality
â‹®----
search_term = st.text_input(
â‹®----
search_type = st.selectbox("Search in:", ["All", "Tables", "Columns"])
â‹®----
@staticmethod
    def render_search_results(analyzer, search_term, search_type)
â‹®----
"""Render schema search results"""
â‹®----
results = analyzer.search_schema(search_term)
â‹®----
# Group columns by table
columns_by_table = {}
â‹®----
table = col['table']
â‹®----
# Show suggestions if no results
â‹®----
@staticmethod
    def render_schema_overview(analyzer, schema)
â‹®----
"""Render complete schema overview"""
â‹®----
# Overview metrics
â‹®----
total_columns = sum(len(table.columns) for table in schema.tables)
â‹®----
total_rows = sum(table.row_count or 0 for table in schema.tables)
â‹®----
fk_count = sum(len(table.get_foreign_keys()) for table in schema.tables)
â‹®----
# Table details
â‹®----
@staticmethod
    def render_table_details(analyzer, table_name)
â‹®----
"""Render detailed table information"""
â‹®----
table = schema.get_table(table_name)
â‹®----
# Table info
â‹®----
pk_columns = [c.name for c in table.get_primary_keys()]
â‹®----
fk_columns = table.get_foreign_keys()
â‹®----
# Columns table
columns_data = []
â‹®----
flags = []
â‹®----
df = pd.DataFrame(columns_data)
â‹®----
# Sample data
â‹®----
sample_df = analyzer.get_sample_data(table_name, limit=5)
â‹®----
# Related tables
related_tables = analyzer.get_related_tables(table_name)
â‹®----
@staticmethod
    def render_query_results_visualization()
â‹®----
"""Enhanced results visualization with charts"""
â‹®----
results = st.session_state.query_results
df = results['data']
â‹®----
# Results header
â‹®----
# Display options
â‹®----
@staticmethod
    def render_data_table(df)
â‹®----
"""Enhanced data table with filtering and pagination"""
â‹®----
# Table controls
â‹®----
# Column filter
all_columns = list(df.columns)
selected_columns = st.multiselect(
â‹®----
default=all_columns[:10]  # Show first 10 columns by default
â‹®----
# Row limit
row_limit = st.selectbox("Rows to display:", [25, 50, 100, 500, "All"])
â‹®----
# Search
search_value = st.text_input("Search in data:", placeholder="Search...")
â‹®----
# Apply filters
display_df = df[selected_columns] if selected_columns else df
â‹®----
# Simple text search across all columns
mask = display_df.astype(str).apply(
display_df = display_df[mask]
â‹®----
display_df = display_df.head(int(row_limit))
â‹®----
# Display table
â‹®----
# Table statistics
â‹®----
@staticmethod
    def render_auto_charts(df)
â‹®----
"""Automatically generate charts based on data types"""
â‹®----
numeric_columns = df.select_dtypes(include=['number']).columns.tolist()
categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()
â‹®----
# Bar chart for categorical data
if categorical_columns and len(df) <= 1000:  # Limit for performance
cat_column = st.selectbox("Categorical column for bar chart:", categorical_columns)
â‹®----
value_counts = df[cat_column].value_counts().head(20)  # Top 20 values
â‹®----
fig = px.bar(
â‹®----
# Histogram for numeric data
â‹®----
num_column = st.selectbox("Numeric column for histogram:", numeric_columns)
â‹®----
fig = px.histogram(
â‹®----
# Additional charts if we have both numeric and categorical
â‹®----
# Scatter plot
â‹®----
x_col = st.selectbox("X-axis:", numeric_columns, key="scatter_x")
y_col = st.selectbox("Y-axis:", [col for col in numeric_columns if col != x_col], key="scatter_y")
color_col = st.selectbox("Color by:", ["None"] + categorical_columns, key="scatter_color")
â‹®----
fig = px.scatter(
â‹®----
# Box plot
â‹®----
cat_col = st.selectbox("Category:", categorical_columns, key="box_cat")
num_col = st.selectbox("Numeric:", numeric_columns, key="box_num")
â‹®----
fig = px.box(
â‹®----
@staticmethod
    def render_export_options(df, results)
â‹®----
"""Export options for query results"""
â‹®----
# Export formats
â‹®----
# CSV Export
csv_data = df.to_csv(index=False)
â‹®----
# JSON Export
json_data = df.to_json(orient='records', indent=2)
â‹®----
# SQL Export (with the query that generated this data)
sql_export = f"""-- Query executed on: {results['timestamp']}
â‹®----
# Export summary
</file>

<file path="utils/__init__.py">
__all__ = [
</file>

<file path="utils/helpers.py">
def format_sql(query: str) -> str
â‹®----
"""Format SQL query for better readability"""
â‹®----
def truncate_text(text: str, max_length: int = 100) -> str
â‹®----
"""Truncate text with ellipsis if too long"""
â‹®----
def safe_execute(func, default_value=None, *args, **kwargs)
â‹®----
"""Safely execute a function with error handling"""
â‹®----
def is_read_only_query(query: str) -> bool
â‹®----
"""Check if query is read-only (SELECT only)"""
query_clean = re.sub(r'--.*$', '', query, flags=re.MULTILINE)
query_clean = re.sub(r'/\*.*?\*/', '', query_clean, flags=re.DOTALL)
query_clean = query_clean.strip().upper()
â‹®----
dangerous_keywords = ['INSERT', 'UPDATE', 'DELETE', 'DROP', 'CREATE', 'ALTER', 'TRUNCATE', 'REPLACE']
â‹®----
def extract_table_names(query: str) -> List[str]
â‹®----
"""Extract table names from SQL query"""
â‹®----
parsed = sqlparse.parse(query)[0]
tables = []
â‹®----
def extract_from_token(token)
â‹®----
def estimate_query_complexity(query: str) -> Dict[str, Any]
â‹®----
"""Estimate query complexity for performance warnings"""
complexity = {
â‹®----
query_upper = query.upper()
â‹®----
# Count JOINs
join_count = query_upper.count('JOIN')
â‹®----
# Count subqueries
subquery_count = query_upper.count('SELECT') - 1  # Subtract main SELECT
â‹®----
# Check for aggregations
agg_functions = ['COUNT', 'SUM', 'AVG', 'MIN', 'MAX', 'GROUP BY']
â‹®----
# Generate warnings
â‹®----
def validate_table_names(query: str, available_tables: List[str]) -> Dict[str, Any]
â‹®----
"""Validate that table names in query exist in schema"""
extracted_tables = extract_table_names(query)
available_lower = [t.lower() for t in available_tables]
â‹®----
validation = {
â‹®----
# Simple similarity matching for suggestions
suggestions = [t for t in available_tables if table.lower() in t.lower()]
â‹®----
def extract_from_token(token)
â‹®----
"""Extract table names from SQL tokens"""
â‹®----
agg_count = sum(1 for func in agg_functions if func in query_upper)
</file>

<file path=".gitignore">
# Environment
.env
.venv/
venv/
env/

# Python
__pycache__/
*.pyc
*.pyo
*.pyd
.Python
*.so
.pytest_cache/

# Database
*.db
*.sqlite
*.sqlite3
connection_cache/

# IDE
.vscode/
.idea/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db

# Logs
*.log
logs/

# Streamlit
.streamlit/
</file>

<file path="app.py">
def main()
â‹®----
"""Main application entry point"""
â‹®----
# Enhanced page configuration
â‹®----
# Custom CSS for professional styling
â‹®----
# Check API configuration
â‹®----
# Initialize session state
â‹®----
# Header with navigation
â‹®----
# Sidebar for database connections and navigation
â‹®----
# Main content area with tabs
â‹®----
def render_main_application()
â‹®----
"""Render the main application interface"""
â‹®----
# Initialize schema analyzer
engine = st.session_state.db_manager.get_active_engine()
â‹®----
# Create main tabs
â‹®----
def render_query_builder()
â‹®----
"""Enhanced query builder interface"""
â‹®----
# Create two columns for input and preview
â‹®----
# Enhanced query input with suggestions
query_placeholder = st.selectbox(
â‹®----
query_placeholder = ""
â‹®----
natural_query = st.text_area(
â‹®----
# Query analysis and generation
â‹®----
# Initialize NL processor
processor = NLProcessor(st.session_state.schema, st.session_state.schema_analyzer)
result = processor.process_query(natural_query)
â‹®----
# Store result in session state
â‹®----
result = st.session_state.current_result
â‹®----
# Display metrics
â‹®----
confidence_color = "green" if result.intent.confidence > 0.8 else "orange" if result.intent.confidence > 0.6 else "red"
â‹®----
complexity_color = "green" if result.intent.complexity_score < 5 else "orange" if result.intent.complexity_score < 8 else "red"
â‹®----
# Query details
â‹®----
# Safety status
â‹®----
# Generated SQL display and execution
â‹®----
# SQL Query section
â‹®----
# Format and display SQL
formatted_sql = format_sql(result.sql_query)
â‹®----
# Show explanation
â‹®----
# Execute button
â‹®----
# Save to favorites
â‹®----
# Copy to clipboard
â‹®----
st.write("SQL copied to clipboard!")  # Note: actual clipboard requires additional setup
â‹®----
# Download query
â‹®----
# Warnings and suggestions
â‹®----
# Optimization suggestions
â‹®----
def execute_query(sql_query: str, natural_query: str)
â‹®----
"""Execute SQL query and display results"""
â‹®----
# Execute query using database manager
df = st.session_state.db_manager.execute_query(sql_query)
â‹®----
# Store results in session state
â‹®----
# Add to query history
â‹®----
# Auto-switch to results tab (would require JavaScript in real implementation)
â‹®----
# Add failed query to history
â‹®----
def save_to_favorites(natural_query: str, sql_query: str)
â‹®----
"""Save query to favorites"""
â‹®----
favorite = {
â‹®----
def render_welcome_screen()
â‹®----
"""Welcome screen when no database is connected"""
â‹®----
# Create and connect to sample database
â‹®----
config = ConnectionConfig(
â‹®----
success = st.session_state.db_manager.add_connection(config)
</file>

<file path="config.py">
# API Configuration - Keep OpenAI for compatibility, add Gemini
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_API_BASE = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
â‹®----
# Google Gemini Configuration
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GEMINI_MODEL = "gemini-1.5-flash"  # Fast and free model
â‹®----
# LLM Configuration - Update to use Gemini
LLM_PROVIDER = "gemini"  # Can be "openai" or "gemini"
LLM_MODEL = GEMINI_MODEL if LLM_PROVIDER == "gemini" else "gpt-4"
LLM_TEMPERATURE = 0.1
MAX_SCHEMA_TOKENS = 3000
â‹®----
# Database Configuration
SUPPORTED_DB_TYPES = ["sqlite", "mysql", "postgresql"]
MAX_QUERY_EXECUTION_TIME = 30  # seconds
MAX_RESULT_ROWS = 10000
DATABASE_ENCRYPTION_KEY = os.getenv("DATABASE_ENCRYPTION_KEY")
â‹®----
# UI Configuration
STREAMLIT_CONFIG = {
â‹®----
# Query Configuration
QUERY_HISTORY_LIMIT = 50
ENABLE_WRITE_OPERATIONS = False  # Safety first
DEFAULT_ROW_LIMIT = 100
</file>

<file path="generate_key.py">
# Generate a proper Fernet encryption key
key = Fernet.generate_key()
</file>

<file path="main.py">
#!/usr/bin/env python3
"""
SQL Query Builder - Command Line Interface
"""
â‹®----
def main()
â‹®----
# Check API key
â‹®----
# Initialize database manager
db_manager = DatabaseManager()
â‹®----
# Create sample database for testing
â‹®----
# Add sample connection
config = ConnectionConfig(
â‹®----
# Analyze schema
engine = db_manager.get_active_engine()
analyzer = SchemaAnalyzer(engine)
schema = analyzer.analyze_database()
â‹®----
for col in table.columns[:3]:  # Show first 3 columns
flags = []
â‹®----
flag_str = f" ({', '.join(flags)})" if flags else ""
</file>

<file path="requirements.txt">
streamlit==1.46.0
sqlalchemy==2.0.41
pandas==2.3.0
python-dotenv==1.1.0
openai==1.90.0
pymysql==1.1.0
psycopg2-binary==2.9.10
sqlparse==0.5.0
sqlglot==25.0.0
plotly==5.24.1
streamlit-aggrid==1.0.5
streamlit-ace==0.1.1
cryptography>=41.0.0
google-generativeai>=0.3.0
</file>

<file path="test_imports.py">

</file>

<file path="test_nl_processor.py">
def test_nl_processor_enhanced()
â‹®----
"""Enhanced test with Phase 2.2 features"""
â‹®----
# Setup
db_manager = DatabaseManager()
â‹®----
config = ConnectionConfig(
â‹®----
# Enhanced setup with schema analyzer
engine = db_manager.get_active_engine()
analyzer = SchemaAnalyzer(engine)
schema = analyzer.analyze_database()
â‹®----
# Create enhanced NL processor
processor = NLProcessor(schema, schema_analyzer=analyzer)
â‹®----
# Test queries with enhanced analysis
test_queries = [
â‹®----
"Show customers who haven't placed any orders"  # New complex query
â‹®----
result = processor.process_query(query)
</file>

<file path="test_phase_2_1.py">
# test_phase_2_1.py
â‹®----
def test_nl_processor_comprehensive()
â‹®----
"""Comprehensive test of our Phase 2.1 NLP engine"""
â‹®----
# Setup - like initializing your app stack
db_manager = DatabaseManager()
â‹®----
# Create sample database - like seeding test data
â‹®----
# Add connection - like configuring database connection - Fix the parameters
config = ConnectionConfig(
â‹®----
database_type=DatabaseType.SQLITE,  # Use DatabaseType enum
db_type="sqlite"  # Keep this as string
â‹®----
success = db_manager.add_connection(config)
â‹®----
# Analyze schema - like discovering API endpoints
engine = db_manager.get_active_engine()
analyzer = SchemaAnalyzer(engine)
schema = analyzer.analyze_database()
â‹®----
# Create NL processor - like initializing your smart API handler
processor = NLProcessor(schema)
â‹®----
# Test different query types - like testing various API endpoints
test_cases = [
â‹®----
# Process query - like making API request
result = processor.process_query(test_case['query'])
â‹®----
# Display results
â‹®----
# Success indicator
status = "âœ… PASS" if result.is_safe and result.sql_query else "âŒ FAIL"
</file>

<file path="test_phase_2_2.py">
# test_phase_2_2.py
â‹®----
def test_phase_2_2_comprehensive()
â‹®----
"""Test Phase 2.2: Schema Context Integration"""
â‹®----
# Setup
db_manager = DatabaseManager()
â‹®----
config = ConnectionConfig(
success = db_manager.add_connection(config)
â‹®----
# Enhanced setup
engine = db_manager.get_active_engine()
analyzer = SchemaAnalyzer(engine)
schema = analyzer.analyze_database()
â‹®----
# Add analyzer to processor for enhanced features
processor = NLProcessor(schema)
processor.schema_analyzer = analyzer  # Enable advanced features
â‹®----
# Advanced test cases
advanced_test_cases = [
â‹®----
result = processor.process_query(test_case['query'])
â‹®----
# Enhanced analysis
â‹®----
# Test new Phase 2.2 features
suggestions = processor.suggest_query_improvements(result.sql_query, result.intent)
â‹®----
for suggestion in suggestions[:2]:  # Show top 2
â‹®----
alternatives = processor.generate_query_alternatives(test_case['query'], result.intent)
â‹®----
for alt in alternatives[:1]:  # Show 1 alternative
â‹®----
# Feature verification
â‹®----
sql_upper = result.sql_query.upper()
â‹®----
feature_upper = feature.upper()
found = any(key in sql_upper for key in feature_upper.split())
status = "âœ…" if found else "âšª"
â‹®----
# Test schema context optimization
â‹®----
# Test token limit management
context_3000 = analyzer.get_schema_context_optimized(max_tokens=3000)
context_1000 = analyzer.get_schema_context_optimized(max_tokens=1000)
â‹®----
# Test schema search
search_results = analyzer.search_schema("customer")
</file>

</files>
